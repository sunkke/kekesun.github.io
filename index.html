<!DOCTYPE html>
<html>

<head>
  <style>
    html {scroll-behavior: smooth;}
  </style>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.2/css/bulma.min.css">
  <link rel="stylesheet" href="css/all.min.css">
  <title>孙科 | Ke Sun</title>
</head>

<body class="has-navbar-fixed-top">

  <nav class="navbar is-fixed-top is-white" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a class="navbar-item" href="index.html">
        <span class="icon-text">
          <span class="icon">
            <i class="fas fa-meteor"></i>
          </span>
          <span><strong>Ke Sun</strong></span>
        </span>
      </a>
  
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
  
    <div id="navbarBasicExample" class="navbar-menu">
      <div class="navbar-start">
        <a class="navbar-item is-outlined" href="#ABOUT">
          About
        </a>
  
        <a class="navbar-item" href="#EXPERIENCE">
          Experience
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More
          </a>
  
          <div class="navbar-dropdown">
            <a class="navbar-item" href="contact.html" target="_blank">
              <span class="icon">
                <i class="fas fa-envelope"></i>
              </span>
              <span class="ml-2">Contact</span>
            </a>
            <a class="navbar-item" href="docs/cv_kesun_en.pdf" target="_blank">
              <span class="icon">
                <i class="fas fa-file"></i>
              </span>
              <span class="ml-2">CV</span>
            </a>
            <a class="navbar-item" href="docs/cv_kesun_zh.pdf" target="_blank">
              <span class="icon">
                <i class="fas fa-file"></i>
              </span>
              <span class="ml-2">简历</span>
            </a>
            <a class="navbar-item" href="https://scholar.google.com/citations?user=_XVBRDQAAAAJ&hl=en&citsig=AMD79or-4TZHNz5VVbR9polX9HcKKq4-KQ" target="_blank">
              <span class="icon">
                <i class="fas fa-graduation-cap"></i>
              </span>
              <span class="ml-2">Google Scholar</span>
            </a>
          </div>
        </div>
      </div>
  
    </div>
  </nav>

  <!-- whole column, 1/12 margin on both sides -->
  
  <div class="columns is-mobile is-gapless is-centered">
    <div class="column is-10-desktop is-9-widescreen is-9-fullhd">
      <!-- About Section -->
      <section class="section" id="ABOUT">
        <h1 class="title is-4">About</h1>

        <div class="columns is-vcentered"> 
          <div class="column is-10 has-text-grey">
            <p class="block" style="text-align:justify;">
              I am a senior Human-Computer Interaction (HCI) researcher and UI designer at Huawei.
              My work focuses on understanding and improving human performance in multiple scenarios, 
              defining platform interaction guidelines and standards, and building new sensing and intelligent interaction technologies. 
              These efforts lie in domains including touchscreen, cross-device, wearable computing, smart home, and intelligent cockpit.
              Some of these work has been shipped with millions of consumer products improving the user experience.
            </p>
            <p class="block" style="text-align:justify;">
              I completed my Ph.D. and B.S. in Computer Science at Tsinghua University. So I often use applied machine
              learning, signal processing, computer vision, interaction design and user experiment in my work.
            </p>
          </div>
          <div class="column is-2">
            <figure class="image" style="width:180px; margin-left:auto; margin-right:auto">
              <img class="is-rounded" src="images/portrait.jpg">
            </figure>
          </div>
        </div>
      </section>

      <!-- Experience Section -->
      <section class="section" id="EXPERIENCE">
        <h1 class="title is-4">Experience</h1>
        
        <div class="tags has-addons">
          <span class="tag is-danger is-light">Huawei</span>
          <span class="tag">2019-present</span>
        </div>
        
        <!-- *** Interaction Unification -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">Interaction Unification for Multiple Devices</h5>
          <!-- video and description -->
          <div>
            <figure class="image" style="margin-left:auto; margin-right:auto">
              <img src="images/hci.jpg">
            </figure>
            <p class="has-text-justified">
              Current mobile devices mainly rely on touchscreen interactions. Beyond smartphones, more and more types of smart devices appear in every digital scenario, such as tablets, PCs, smart wearables, televisions, vehicle-mounted devices, virtual reality (VR), and augmented reality (AR). An application may run on multiple types of devices. And users may adopt diversified input modes to interact with the application on a device. Therefore, the user interface must be able to automatically identify and support different input devices, and allow users to interact with the application with ease and as expected. We call this the ability of interaction unification. As the design lead of this feature, I:
            </p>
            <div class="content">
              <ol type="1">
                <li><strong>Make the Human-Computer Interaction design guidelines for Harmony OS. </strong>
                  I defined the characteristics, functions and interaction rules for various interaction modes and input devices. 
                  For each interaction task, I sorted out the standard input interaction behaviors when using various input devices, 
                  that is, to normalize different interaction events to the same standard interaction event. 
                  A brief version can be accessed at <a href="https://developer.harmonyos.com/en/docs/design/des-guides/hci-overview-0000001054726474"  target="_blank">Harmonyos.com</a>.</li>
                <li><strong>Modify all the UI controls of HarmonyOS to have the ability of interaction unification.</strong> 
                  To implement the above design specifications, I was deeply involved in the development of the operating system on both the framework input subsystem and the control side. 
                  I provided technical solutions and helped write the development guide. 
                  As a result, we achieve “development once, multi-device effective” for interaction and ultimately improve the consistency of user experience.</li>
                <li><strong>Try to make the above specifications an International Standard.</strong> 
                  I wrote the proposal of “Mapping framework of interaction events across devices” to ISO. 
                  The proposal has passed the domestic review and is being reviewed at the ISO’s WG9 meeting.</li>
                <li><strong>Define full-scene multi-touch gestures for HarmonyOS.</strong> 
                  design all characteristic gestures for touch interaction in Huawei ecological devices, including but not limited to mobile phone/tablet, PC's touchpad and mouse, smart cockpit, cross-screen/cross-device, etc.</li>
              </ol>
            </div>
          </div>
          <!-- notifications -->
          <div class="notification mt-4">
            Interaction Unification is a system-level capability that allows an app to automatically support a variety of input devices / modes with no or few adaptations.
            For example, recent released 
            <span class="icon-text">
              <span class="icon">
                <i class="fas fa-tablet-alt"></i>
              </span>
              <span><a href="https://www.vmall.com/product/10086366915386.html" target="_blank">MatePad</a> </span>
            </span>
            and
            <span class="icon-text">
              <span class="icon">
                <i class="fas fa-desktop"></i>
              </span>
              <span><a href="https://www.vmall.com/product/10086690832694.html#10086836193769" target="_blank">MateView</a>  </span>
            </span>
             are featured with experience improvement in mouse and keyboard interaction.
          </div>
        </div>

        <!-- *** Novel Interaction Techniques -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;"> Novel Interaction Techniques</h5>
          <!-- video and description -->
          <div>
            <p class="has-text-justified">
              This part involves low level empirical studies to improve the understanding of the human factors, and the design and implementation of novel techniques and interfaces to provide natural and enhanced user experiences. Topics include: touch gestures, cross-device interactions, wearable and spatial interactions in smart home, and user interfaces for automotive intelligent cockpit. Some of the work has been included into product planning and will be reflected in future products.  
            </p>
          </div>
        </div>

        <!-- *** Sparse keyboard -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">Accessibility</h5>
          <!-- video and description -->
          <div class="columns mb-0"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <figure class="image">
                <img src="images/accessibility.png">
              </figure>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p style="text-align:justify;">
                I wrote the “Accessibility Screen Reader Development Specification”, 
                which became Huawei’s internal software development standard and has been integrated into the IPD process. 
                I also empowered all developers, designers, and test engineers. As a core member of the accessibility special team, 
                I participated in the improvement of 1000+ problems. 
                As a result, Huawei ranked 1st among domestic mobile phones in 
                <a href="https://tech.ifeng.com/c/82PZhbIXVvD" target="_blank"> IFENG.COM’s 2021 accessibility evaluation with Shenzhen Accessibility Research Association.</a>
              </p>
            </div>
          </div>
        </div>
        <!-- End of Sparse Keyboard -->



        <div class="tags has-addons mt-6">
          <span class="tag is-link is-light">Tsinghua University</span>
          <span class="tag">2010-2019</span>
        </div>

        <!-- *** Float -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">Float: one-handed and touch-free target selection on smartwatches</h5>
          <!-- video and description -->
          <div class="columns mb-0"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <video autoplay loop muted playsinline>
                <source src="videos/float.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p class="has-text-justified">
                Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the "fat finger" problem.
                Float is a wrist-to-finger input approach that enables one-handed and touch-free interaction on the smartwatch with high efficiency and precision using only commercially-available built-in sensors.
                A user tilts the wrist to point and performs an in-air finger tap to click. 
                To detect hand gestures, we are the first to combine the photoplethysmogram (PPG) signal from the optical heart rate monitor with the accelerometer and gyroscope from the IMU.
              </p>
            </div>
          </div>
          <!-- link buttons and publication info -->
          <nav class="level">
            <!-- Left side -->
            <div class="level-left">
              <div class="level-item">
                <a class="button is-small is-info is-inverted" href="http://pi.cs.tsinghua.edu.cn/lab/papers/Float_Ke%20Sun_CHI2017.pdf" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0"  href="https://www.youtube.com/watch?v=CVftmKiMF7Y" target="_blank">
                  <span class="icon is-small">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0"  href="https://dl.acm.org/doi/10.1145/3025453.3026027" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-link"></i>
                  </span>
                  <span>DOI</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0"  href="https://www.youtube.com/watch?v=tgdaKOGZbTo" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  <span>Talk</span>
                </a>
              </div>
            </div>
            <!-- Right side -->
            <div class="level-right">
              <p class="level-item is-size-7 has-text-info">
                Published at CHI 2017
              </p>
            </div>
          </nav>
          <!-- notifications -->
          <div class="notification">
            Glad to see that the idea and solution of Float - tilting wrist to point and using heart rate sensor + motion sensor to detect hand actions- 
            have also been adopted exactly by Apple to launch the feature <a href="https://www.apple.com/newsroom/2021/05/apple-previews-powerful-software-updates-designed-for-people-with-disabilities/" target="_blank">AssistiveTouch for Apple Watch</a>.
          </div>
        </div>

        <!-- *** Lip-Interact -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands</h5>
          <!-- video and description -->
          <div class="columns mb-0"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <video autoplay loop muted playsinline>
                <source src="videos/lipinteract.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p style="text-align:justify;">
                Lip-Interact is an interaction technique that allows users to issue commands on their smartphone through silent speech. 
                Lip-Interact repurposes the front camera to capture the user's mouth movements and recognize commands with a deep learning model. 
                Our system supports 44 commands for accessing both system-level functionalities (launching apps, changing system settings) and application-level functionalities. 
                Lip-Interact help users access functionality efficiently in one step, enable one-handed input, and assist touch to make interactions more fluent.
              </p>
            </div>
          </div>
          <!-- link buttons and publication info -->
          <nav class="level">
            <!-- Left side -->
            <div class="level-left">
              <div class="level-item">
                <a class="button is-small is-info is-inverted" href="http://pi.cs.tsinghua.edu.cn/lab/papers/Lip_Interact_final.pdf" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://www.youtube.com/watch?v=3BkYloAlcr8" target="_blank">
                  <span class="icon is-small">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://dl.acm.org/doi/abs/10.1145/3242587.3242599" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-link"></i>
                  </span>
                  <span>DOI</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://www.youtube.com/watch?v=pYCQWlkDDK8" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  <span>Talk</span>
                </a>
              </div>
            </div>
            <!-- Right side -->
            <div class="level-right">
              <p class="level-item is-size-7 has-text-info">
                Published at UIST 2018
              </p>
            </div>
          </nav>
        </div>

        <!-- *** 1D-Handwriting -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">One-Dimensional Handwriting: Inputting Letters and Words on Smart Glasses</h5>
          <!-- video and description -->
          <div class="columns mb-0"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <video autoplay loop muted playsinline>
                <source src="videos/1d.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p style="text-align:justify;">
                1D Handwriting is a unistroke gesture technique enabling text entry on a one-dimensional interface.
                The challenge is to map two-dimensional handwriting to a reduced one-dimensional space, while achieving a balance between memorability and performance efficiency. 
                After an iterative design, we finally derive a set of ambiguous two-length unistroke gestures.
                Users studies show that 1D Handwriting significantly outperforms a selection-based technique  for both letter input and word input. With extensive training, text entry rate can reach 19.6 WPM.
              </p>
            </div>
          </div>
          <!-- link buttons and publication info -->
          <nav class="level">
            <!-- Left side -->
            <div class="level-left">
              <div class="level-item">
                <a class="button is-small is-info is-inverted" href="http://pi.cs.tsinghua.edu.cn/lab/papers/p71-yu.pdf" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://www.youtube.com/watch?v=l9FIXMPrjF8" target="_blank">
                  <span class="icon is-small">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://dl.acm.org/doi/10.1145/2858036.2858542" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-link"></i>
                  </span>
                  <span>DOI</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://www.youtube.com/watch?v=mNtn3iIGwBU" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  <span>Talk</span>
                </a>
              </div>
            </div>
            <!-- Right side -->
            <div class="level-right">
              <span class="level-item icon-text is-size-7 has-text-info">
                <span>Published at CHI 2016,</span>
                <span class="icon">
                  <i class="fas fa-award"></i>
                </span>
                <span>Honorable Mention Award</span>
              </span>
            </div>
          </nav>
        </div>
        <!-- End of 1D handwriting -->

        <!-- *** Sparse keyboard -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">Exploring Low-Occlusion Qwerty Soft Keyboard Using Spatial Landmarks</h5>
          <!-- video and description -->
          <div class="columns mb-0 is-vcentered"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <figure class="image">
                <img src="images/sparse.png">
              </figure>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p style="text-align:justify;">
                Qwerty soft keyboards often consume a large portion of the touchscreen space, occluding the application view on the smartphone and requiring a separate input interface on the smartwatch. 
                To free up the screen real estate, we explore the concept of Sparse Keyboard and proposes two new ways of presenting the Qwerty soft keyboard. 
                The idea is to use users’ spatial memory and the reference effect of spatial landmarks on the graphical interface. 
                Our final design K3-SGK displays only three keys while L5-EYOCN displays only five line segments instead of the entire Qwerty layout. 
                To achieve this, we employ a user-centered computational design method: first study the reference effect of a single landmark key (line segment) from empirical data, then make assumptions to generalize the effect to multiple landmarks, and finally optimize the best designs.
              </p>
            </div>
          </div>
          <!-- link buttons and publication info -->
          <nav class="level">
            <!-- Left side -->
            <div class="level-left">
              <div class="level-item">
                <a class="button is-small is-info is-inverted" href="http://pi.cs.tsinghua.edu.cn/lab/papers/Exploring%20Low-Occlusion%20Qwerty%20Soft%20Keyboard%20Using%20Spatial%20Landmarks.pdf" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://dl.acm.org/doi/fullHtml/10.1145/3318141" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-link"></i>
                  </span>
                  <span>DOI</span>
                </a>
              </div>
            </div>
            <!-- Right side -->
            <div class="level-right">
              <p class="level-item is-size-7 has-text-info">
                Published at TOCHI 2019
              </p>
            </div>
          </nav>
        </div>
        <!-- End of Sparse Keyboard -->

        <!-- *** ATK -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data</h5>
          <!-- video and description -->
          <div class="columns mb-0"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <video autoplay loop muted playsinline>
                <source src="videos/atk.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p style="text-align:justify;">
                ATK is a novel interaction technique that enables freehand ten-finger typing in the air based on 3D hand tracking data. 
                We followed an iterative approach in designing ATK. We first empirically investigated users' mid-air typing behavior, 
                and examined fingertip kinematics during tapping, correlated movement among fingers and 3D distribution of tapping endpoints. 
                Based on the findings, we proposed a probabilistic tap detection algorithm, 
                and augmented Goodman's input correction model to account for the ambiguity in distinguishing tapping finger.
              </p>
            </div>
          </div>
          <!-- link buttons and publication info -->
          <nav class="level">
            <!-- Left side -->
            <div class="level-left">
              <div class="level-item">
                <a class="button is-small is-info is-inverted" href="http://pi.cs.tsinghua.edu.cn/lab/papers/p539-yi.pdf" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://www.youtube.com/watch?v=gYkSOzKY1LQ" target="_blank">
                  <span class="icon is-small">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://dl.acm.org/doi/abs/10.1145/2807442.2807504" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-link"></i>
                  </span>
                  <span>DOI</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://www.youtube.com/watch?v=miD5xzQ208I" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  <span>Talk</span>
                </a>
              </div>
            </div>
            <!-- Right side -->
            <div class="level-right">
              <p class="level-item is-size-7 has-text-info">
                Published at UIST 2015
              </p>
            </div>
          </nav>
        </div>
        <!-- End of ATK -->

        <!-- *** ThermalRing -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">ThermalRing: Gesture and Tag Inputs Enabled by a Thermal Imaging Smart Ring</h5>
          <!-- video and description -->
          <div class="columns mb-0"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <video autoplay loop muted playsinline>
                <source src="videos/ThermalRing.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p style="text-align:justify;">
                ThermalRing is a thermal imaging smart ring using low-resolution thermal camera for identity-anonymous, illumination-invariant, and power-efficient sensing of both dynamic and static gestures. 
                We also design ThermalTag, thin and passive thermal imageable tags that reflect the heat from the human hand. 
                ThermalTag can be easily made and applied onto everyday objects by users. 
                We develop sensing techniques for three typical input demands:drawing gestures for device pairing, click and slide gestures for device control, and tag scan gestures for quick access. 
              </p>
            </div>
          </div>
          <!-- link buttons and publication info -->
          <nav class="level">
            <!-- Left side -->
            <div class="level-left">
              <div class="level-item">
                <a class="button is-small is-info is-inverted" href="https://txzhang.me/publication/thermalring20/thermalring.pdf" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://www.youtube.com/watch?v=FL_L3GMQlwc" target="_blank">
                  <span class="icon is-small">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376323" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-link"></i>
                  </span>
                  <span>DOI</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://www.youtube.com/watch?v=1dF08kR4yqg" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  <span>Talk</span>
                </a>
              </div>
            </div>
            <!-- Right side -->
            <div class="level-right">
              <p class="level-item is-size-7 has-text-info">
                Published at CHI 2019
              </p>
            </div>
          </nav>
        </div>
        <!-- End of ThermalRing -->

        <!-- *** Virtual Grasp -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval</h5>
          <!-- video and description -->
          <div class="columns mb-0"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <figure class="image">
                <img src="images/virtualgrasp.png">
              </figure>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p style="text-align:justify;">
                VirtualGrasp is a novel gestural approach to retrieve virtual objects in virtual reality. 
                Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. 
                The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. 
                Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. 
              </p>
            </div>
          </div>
          <!-- link buttons and publication info -->
          <nav class="level">
            <!-- Left side -->
            <div class="level-left">
              <div class="level-item">
                <a class="button is-small is-info is-inverted"  href="http://pi.cs.tsinghua.edu.cn/lab/papers/VirtualGrasp.pdf" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://dl.acm.org/doi/10.1145/3173574.3173652" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-link"></i>
                  </span>
                  <span>DOI</span>
                </a>
              </div>
            </div>
            <!-- Right side -->
            <div class="level-right">
              <p class="level-item is-size-7 has-text-info">
                Published at CHI 2018
              </p>
            </div>
          </nav>
        </div>
        <!-- End of Virtual grasp -->

        <!-- *** Skin Motion -->
        <div class="box">
          <!-- project paper title -->
          <h5 class="title is-5" style="color:Coral;">SkinMotion: what does skin movement tell us?</h5>
          <!-- video and description -->
          <div class="columns mb-0"> 
            <div class="column is-4 is-4-desktop is-3-widescreen is-3-fullhd has-background-white">
              <figure class="image">
                <img src="images/skinmotion.png">
              </figure>
            </div>
            <div class="column is-8 is-8-desktop is-9-widescreen is-9-fullhd has-background-white	">
              <p style="text-align:justify;">
                With the increasing popularity of wearable computing, 
                emerging techniques allow novel interaction modalities to be transferred from portable devices to the human body itself. 
                We show an alternative interaction modality - SkinMotion.
                SkinMotion reconstructs human motions from skin-stretching movements. 
                We discuss the potential applications of SkinMotion. 
                In addition, we explore one specific instance -- finger motion detection using the skin movement on the dorsum of the hand. 
              </p>
            </div>
          </div>
          <!-- link buttons and publication info -->
          <nav class="level">
            <!-- Left side -->
            <div class="level-left">
              <div class="level-item">
                <a class="button is-small is-info is-inverted" href="https://acsweb.ucsd.edu/~l5sun/files/Ubicomp2016_Workshop.pdf" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
                <a class="button is-small is-info is-inverted ml-0" href="https://dl.acm.org/doi/abs/10.1145/2968219.2979132" target="_blank">
                  <span class="icon is-small">
                    <i class="fas fa-link"></i>
                  </span>
                  <span>DOI</span>
                </a>
              </div>
            </div>
            <!-- Right side -->
            <div class="level-right">
              <p class="level-item is-size-7 has-text-info">
                Published at UbiComp 2016 (Workshop)  
              </p>
            </div>
          </nav>
        </div>
        <!-- End of Skin Motion -->
      
      <!-- Experience Section End -->
      </section>

      <!-- About Section -->
      <section class="section">
        <div>
          <p class="has-text-grey">
            Updated by 2021.07 
          </p>
        </div>
      </section>
    <!-- Main Column End -->
    </div>
  
  <!-- whole column end -->
  </div> 

</body>
</html>